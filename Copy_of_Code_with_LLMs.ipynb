{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with LLMs!\n",
        "\n",
        "In this notebook, we will cover all necessary ingredients to working with LLMs using code\n",
        "\n",
        "1. Installing Libraries\n",
        "2. Using LLMs using APIs\n",
        "3. Prompt Templates and Chains\n",
        "4. Prompt Engineering Techniques"
      ],
      "metadata": {
        "id": "8xgmhFrxFdZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Python"
      ],
      "metadata": {
        "id": "-g3CP6NNEcwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2\n",
        "b = 3\n",
        "\n",
        "print(a+b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDlY3N8GEL6A",
        "outputId": "8038d3fc-e58f-4944-bb89-3ab92178fa70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"Satvik\"\n",
        "\n",
        "print(f\"Hello {name}, Welcome to AI Prototyping Session!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGqjpNmDEQSr",
        "outputId": "9f52b0da-45ba-4c50-befd-b6706f92a27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Satvik, Welcome to AI Prototyping Session!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-sh-8ySTM2"
      },
      "source": [
        "## Installing libraries\n",
        "\n",
        "\n",
        "Installing essential Python libraries who allows you to work with LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La0JvboxmdD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a8f5e4-4e7a-4205-f386-8f44f5773610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/415.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URfJBVwtq4Dp"
      },
      "source": [
        "\n",
        "### Storing API keys\n",
        "\n",
        "API keys allow you access LLMs by the providers (OpenAI, Google, Anthropic, etc). In this lecture, we will be using OpenAI models (gpt-3.5-turbo, gpt4) and Google's LLMs (Gemini models)\n",
        "\n",
        "- OpenAI API key: https://platform.openai.com/account/api-keys\n",
        "- Google API key: https://aistudio.google.com (FREE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx_d9XJFm77k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up API keys for OpenAI and Google\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14mklUIFrP5A"
      },
      "source": [
        "### Using LLM to run a query using API\n",
        "\n",
        "In this code, you will understand how to send a prompt to an LLM via an API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBteLVmqnOCd"
      },
      "outputs": [],
      "source": [
        "# Import ChatOpenAI module\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize OpenAI's GPT-4o model\n",
        "gpt4o_model = ChatOpenAI(model_name = \"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4o_model.invoke(\"Hello, how are you?\")"
      ],
      "metadata": {
        "id": "pGkOcqFCdYCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2041b8d-fbce-4c72-8c8c-926215cfaacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 13, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-1f62417e-d995-4a8f-819a-c83978c73a62-0', usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using the GPT-4o model\n",
        "response = gpt4o_model.invoke(\"Generate 3 tweets roasting Product Managers\")\n",
        "\n",
        "# Display the output\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "GiGYMiICpo9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da395802-b15b-4095-d659-3401f62e9075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. \"Product Managers: The only people who can take a simple idea, hold 47 meetings, create 13 PowerPoints, and turn it into a project that takes 6 months and 10 engineers to complete. #EfficiencyWho?\"\n",
            "\n",
            "2. \"A moment of silence for all the great product ideas that died in meetings. Product Managers, you have a gift for transforming innovation into eternal PowerPoints. #RIPGreatIdeas\"\n",
            "\n",
            "3. \"Product Managers be like, 'We need a meeting to summarize the last meeting, which was actually about what to discuss in the upcoming meeting.' At this point, they should just sell tickets. 🎟️ #MeetingCulture\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using the GPT-4o model\n",
        "response = gpt4o_model.invoke(\"\")\n",
        "\n",
        "# Display the output\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "lqzgj0Px-oCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Open Source models using Groq"
      ],
      "metadata": {
        "id": "rBQLmMA--v9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing libraries\n",
        "\n",
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_vxgBZD-vwI",
        "outputId": "9fc83cb6-9c42-4ab6-8bb8-eedca1ef69c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Groq API key: https://console.groq.com/keys (FREE)"
      ],
      "metadata": {
        "id": "JY3N1Hg6_wJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing API keys\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "li6emBOb-vtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "llm.invoke(\"Who are you, bruh?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4PuTw_T-vp5",
        "outputId": "245a237c-82e5-46da-b24c-1f817b2475b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm an AI assistant, which means I'm a computer program designed to simulate conversations, answer questions, and provide information on a wide range of topics. I'm here to help with any questions or topics you'd like to discuss. I don't have a personal identity or emotions like a human would, but I'll do my best to provide helpful and accurate responses. What's on your mind, bruh?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 42, 'total_tokens': 126, 'completion_time': 0.305454545, 'prompt_time': 0.004608931, 'queue_time': 0.234769384, 'total_time': 0.310063476}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_5f849c5a0b', 'finish_reason': 'stop', 'logprobs': None}, id='run-48b7987d-bbf7-4ac3-8848-77af74923af4-0', usage_metadata={'input_tokens': 42, 'output_tokens': 84, 'total_tokens': 126})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try a Deepseek Model\n",
        "\n",
        "## ??"
      ],
      "metadata": {
        "id": "poy4Qo3hFrzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Act as a therapist, be kind, give short response. I am feeling lonely. \")"
      ],
      "metadata": {
        "id": "L_F0U0d5Giv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Gemini Models (homework?)"
      ],
      "metadata": {
        "id": "nIAnh1GvSd5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: [Langchain Google Docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)"
      ],
      "metadata": {
        "id": "aMivkqXmDjNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0P6tlqiEMtC"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZAfJwFctDrp"
      },
      "source": [
        "### Using Prompt Template\n",
        "\n",
        "Prompt templates are pre-designed patterns for creating prompts, with placeholders for specific inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISLTNzK8qWfw"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create prompt template for generating tweets\n",
        "\n",
        "tweet_template = \"Give me {number} tweets on {topic} in Hindi\"\n",
        "\n",
        "tweet_prompt = PromptTemplate(template = tweet_template, input_variables = ['number', 'topic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Yp6sOzt7ED"
      },
      "outputs": [],
      "source": [
        "tweet_template.format(number = 7, topic = \"Submarine\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yypnESQuuqRs"
      },
      "source": [
        "### Using LLM Chains\n",
        "\n",
        "LLM Chains are sequences of prompts and language models combined to perform more complex tasks.\n",
        "\n",
        "LLM Chain = Prompt Template | LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k02dkG3uuPPU"
      },
      "outputs": [],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create LLM chain using the prompt template and model\n",
        "tweet_chain = tweet_prompt | gpt4o_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using the LLM chain\n",
        "response = tweet_chain.invoke({\"number\" : 5, \"topic\" : \"Wars in Middle East\"})\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "cS78bwxx43ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using the LLM chain\n",
        "\n",
        "response = tweet_chain.invoke({\"number\" : 10, \"topic\" : \"Road Safety\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "sGEK7NAvk3QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3l23b_WHXlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Own Prompt Templates\n",
        "\n",
        "Try: Create an email template where you can enter - subject, sender, and tone."
      ],
      "metadata": {
        "id": "qggu_EdLG3Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code\n",
        "\n"
      ],
      "metadata": {
        "id": "4IWnUZFtHX4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEh8d5OSZEeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpEA7rKsZEW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSjt6y42Ijzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy your First AI APP!"
      ],
      "metadata": {
        "id": "s80Euby32GxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1:\n",
        "\n",
        "Create a Github repo\n",
        "\n",
        "STEP 2:\n",
        "\n",
        "Create requirements.txt and test.py\n",
        "\n",
        "(Link to code: https://github.com/satvik314/basic_app_template)\n",
        "\n",
        "STEP 3:\n",
        "\n",
        "Create an account on Streamlit (you will have to link Github)\n",
        "\n",
        "STEP 4:\n",
        "\n",
        "Go to My Apps > Create App > Paste Github URL > Type name of your app > Save API key in Advanced Settings\n",
        "\n",
        "STEP 5:\n",
        "\n",
        "DEPLOY!!"
      ],
      "metadata": {
        "id": "hiJn0RDUbu_J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mwV8pSfScOnk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}